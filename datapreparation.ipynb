{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Machine%20Learning/Desktop/workspace-wildAI/HerdNet\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Installing collected packages: animaloc\n",
      "  Attempting uninstall: animaloc\n",
      "    Found existing installation: animaloc 0.2.0\n",
      "    Uninstalling animaloc-0.2.0:\n",
      "      Successfully uninstalled animaloc-0.2.0\n",
      "  Running setup.py develop for animaloc\n",
      "Successfully installed animaloc-0.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\users\\machine learning\\anaconda3\\envs\\herdnet\\lib\\site-packages\\animaloc-0.2.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "#Uncomment below to install animaloc\n",
    "!pip install -e . --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import PIL\n",
    "import torchvision\n",
    "import numpy\n",
    "import cv2\n",
    "import skimage\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from albumentations import PadIfNeeded\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from animaloc.data import ImageToPatches, PatchesBuffer, save_batch_images\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiling data for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For GeoTiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patches(img_path:Path,dest_dir:Path,tilesize=512):\n",
    "\n",
    "    # create directory\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)\n",
    "        os.mkdir(dest_dir)\n",
    "        print(\"emptying directory:\", dest_dir)\n",
    "    else:\n",
    "        os.mkdir(dest_dir)\n",
    "        print(\"creating directory:\", dest_dir)\n",
    "    \n",
    "    # window reading with rasterio\n",
    "    handler = rasterio.open(img_path)\n",
    "    height, width = handler.meta['height'], handler.meta['width']\n",
    "    coordinates = dict()\n",
    "    count = 0\n",
    "    for i,j in tqdm(product(list(range(0,height,tilesize)),list(range(0,width,tilesize)))):\n",
    "        window = Window(j, i, tilesize, tilesize)\n",
    "        \n",
    "        try:\n",
    "            chunk = handler.read(window=window)\n",
    "            c,h,w = chunk.shape\n",
    "            xmin, xmax = j, j+w\n",
    "            ymin, ymax = i, i+h\n",
    "            x_center = 0.5*(xmin+xmax)\n",
    "            y_center = 0.5*(ymin+ymax)\n",
    "            n_unique = np.unique(chunk).size\n",
    "            if n_unique == 1:\n",
    "                continue\n",
    "            count += 1\n",
    "            filename = img_path.name.split('.')[0] + f\"-{j}-{i}.png\"\n",
    "            coordinates[count] = [xmin,xmax,ymin,ymax,x_center,y_center,filename]\n",
    "\n",
    "            # save to disk\n",
    "            chunk = np.transpose(chunk,(1,2,0))\n",
    "            skimage.io.imsave(dest_dir/filename,chunk)          \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Failed for\",(i,j),e)\n",
    "            pass\n",
    "\n",
    "    cols = ['xmin','xmax','ymin','ymax','x_center','y_center','filename']\n",
    "    coordinates = pd.DataFrame.from_dict(coordinates,\n",
    "                                        orient='index',\n",
    "                                        columns=cols)\n",
    "    coordinates.to_csv(dest_dir/f\"coordinates{img_path.name.split('.')[0]}.csv\",index=False)\n",
    "    handler.close()\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for datapath in Path(\"../annotation_data/camp6/\").iterdir():\n",
    "datapath = Path(\"../annotation_data/camp6/150m_RGB.tif\")\n",
    "dest_dir = datapath.parent/(datapath.name.split('.')[0])\n",
    "# coordinates =  save_patches(datapath,dest_dir,tilesize=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Images (jpg, rgb etc.)\n",
    "Using Herdnet code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/HerdNet\n"
     ]
    }
   ],
   "source": [
    "!pwd # current working dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meanings of arguments\n",
    "- ```-ratioheight``` : proportion of tile  w.r.t height of image. Example 0.5 means dividing the image in two bands w.r.t height. If it's 1.0 then this parameters is disabled.\n",
    "- ```-ratiowidth``` : proportion of tile w.r.t to width of image. Example 0.9999 means the width of the tile is the same as the image. If it's 1.0 then this parameters is disabled.\n",
    "- ```-overlapfactor``` : percentage of overlap. It should be less than 1.\n",
    "- ```-rmheight``` : percentage of height to remove or crop at bottom and top\n",
    "- ```-rmwidth``` : percentage of width to remove or crop on each side of the image\n",
    "- ```-pattern``` : \"**/*.JPG\" will get all .JPG images in directory and subdirectories. On windows it will get both .JPG and .jpg. On unix it will only get .JPG images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiling Paul's data \n",
    "# Path(r\"D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Farm-tiled\").mkdir(exist_ok=True,parents=True)\n",
    "!python ./tools/patcher.py \"D:\\PhD\\Data per camp\\Dry season\\Kapiri August test\\DJI_202310011649_001_Kapiri-6-7-8-55m\" 0 0 0 -ratiowidth 0.34 -ratioheight 0.5 -dest \"D:\\PhD\\Data per camp\\Dry season\\Kapiri August test\\kapiri-6-7-8-55m_tiled\"  -min 0.0 -all True -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiling Paul's data \n",
    "# Path(r\"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 1\\Rep 1 - tiled\").mkdir(exist_ok=True,parents=True)\n",
    "# !python ./tools/patcher.py \"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 2\\Rep 1\" 0 0 0  -ratiowidth 1.0 -ratioheight 1.0 -rmheight 0.22 -rmwidth 0.28 -dest \"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 2\\Rep 1 - tiled\"  -min 0.0 -all True -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to run\n",
    "##!python ./tools/patcher.py \"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 3\\Rep 1\" 0 0 0  -ratiowidth 1.0 -ratioheight 1.0 -rmheight 0.22 -rmwidth 0.28 -dest \"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 3\\Rep 1 - tiled\"  -min 0.0 -all True -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to run\n",
    "##!python ./tools/patcher.py \"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 9_11\\Rep 1\" 0 0 0  -ratiowidth 0.5 -ratioheight 0.5 -rmheight 0.22 -rmwidth 0.28 -dest \"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 9_11\\Rep 1 - tiled\"  -min 0.0 -all True -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to run\n",
    "##!python ./tools/patcher.py \"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 4_5\\Rep 1\" 0 0 0  -ratiowidth 0.5 -ratioheight 0.99999 -rmheight 0.22 -rmwidth 0.28 -dest \"D:\\PhD\\Data per camp\\Wet season\\Kapiri\\Camp 4_5\\Rep 1 - tiled\"  -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New script for tiling data\n",
    "!python ./tools/patcher.py \"D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 2\\extra data\\DJI_202310011415_004_Camp2\" 0 0 0 -overlapfactor 0.2  -ratiowidth 0.5 -ratioheight 1.0 -rmheight 0.22 -rmwidth 0.28 -dest \"D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 2\\extra data - tiled\"  -pattern \"**/*.JPG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0, 450), (0, 1000)), ((360, 810), (0, 1000)), ((720, 1170), (0, 1000)), ((1080, 1530), (0, 1000)), ((1440, 2000), (0, 1000))]\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_patches(image,image_width:int,image_height:int,tile_w:int,tile_h:int,overlaping_factor:float):\n",
    "    patches = list()\n",
    "\n",
    "    def get_coordinates():\n",
    "\n",
    "        # x limits\n",
    "        lim = math.ceil((image_width-tile_w)/((1-overlaping_factor)*tile_w))\n",
    "        x_right = [math.floor(tile_w + i*(1-overlaping_factor)*tile_w) for i in range(lim)]\n",
    "        x_coords = [(x-tile_w,x) for x in x_right]\n",
    "        if len(x_coords)>0:\n",
    "            left,right = x_coords[-1]\n",
    "            x_coords[-1] = (left,image_width) # extending to remaining pixels\n",
    "\n",
    "        # y limits\n",
    "        lim = math.ceil((image_height-tile_h)/((1-overlaping_factor)*tile_h))\n",
    "        y_bottom = [math.floor(tile_h + i*(1-overlaping_factor)*tile_h) for i in range(lim)]\n",
    "        y_coords = [(y-tile_h,y) for y in y_bottom]\n",
    "        if len(y_coords)>0:\n",
    "            top,bottom = y_coords[-1]\n",
    "            y_coords[-1] = (top,image_height) # extending to remaining pixels\n",
    "\n",
    "        # tiles coordinates\n",
    "        if len(y_coords)>0 and len(x_coords)>0:\n",
    "            pass\n",
    "        elif len(y_coords) == 0:\n",
    "            y_coords = [(0,image_height),]\n",
    "        elif len(x_coords) == 0:\n",
    "            x_coords = [(0,image_width),]\n",
    "\n",
    "        coordinates = product(x_coords,y_coords)\n",
    "        return list(coordinates)\n",
    "    \n",
    "    coords =  get_coordinates()\n",
    "    for (x_left,x_right),(y_top,y_bottom) in coords:\n",
    "        patches.append(image[:,y_top:y_bottom,x_left:x_right])\n",
    "    \n",
    "    print(coords)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "img = np.zeros((3,1000,2000))\n",
    "patches = get_patches(img,image_width=img.shape[2],image_height=img.shape[1],tile_w=450,tile_h=1000,overlaping_factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1000, 450),\n",
       " (3, 1000, 450),\n",
       " (3, 1000, 450),\n",
       " (3, 1000, 450),\n",
       " (3, 1000, 560)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.shape for p in patches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiling validation data\n",
    "Path(\"../general_dataset/val_splits\").mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "!python ./tools/patcher.py ../general_dataset/val 640 640 100 \\\n",
    "    ../general_dataset/val_splits \\\n",
    "    -csv ../general_dataset/groundtruth/csv/val_big_size_A_B_E_K_WH_WB.csv \\\n",
    "    -min 0.0 -all False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiling training data\n",
    "Path(\"../general_dataset/train_splits\").mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "!python ./tools/patcher.py ../general_dataset/train 640 640 100 \\\n",
    "    ../general_dataset/train_splits \\\n",
    "    -csv ../general_dataset/groundtruth/csv/train_big_size_A_B_E_K_WH_WB.csv \\\n",
    "    -min 0.0 -all False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiling test data\n",
    "Path(\"../general_dataset/test_splits\").mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "!python ./tools/patcher.py ../general_dataset/test 640 640 100 \\\n",
    "    ../general_dataset/test_splits \\\n",
    "    -csv ../general_dataset/groundtruth/csv/test_big_size_A_B_E_K_WH_WB.csv \\\n",
    "    -min 0.0 -all False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting patches: 100%|██████████████████| 1226/1226 [1:02:52<00:00,  3.08s/it]\n"
     ]
    }
   ],
   "source": [
    "# tiling Annotation data\n",
    "\n",
    "# Destination of splits\n",
    "Path(\"../EBP-Lindanda-cam0-splits\").mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "!python ./tools/patcher.py ../EBP-Lindanda-cam0 512 512 64 \\\n",
    "    ../EBP-Lindanda-cam0-splits \\\n",
    "    -min 0.0 -all True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tiling data from Savmap dataset\n",
    "# path_gt = '../savmap_dataset_v2/gt.csv'\n",
    "# df_gt = pd.read_csv(path_gt)\n",
    "# df_gt['labels'] = 0\n",
    "# df_gt.rename(columns={'filename':'images',\n",
    "#                       'xmin':'x_min',\n",
    "#                       'xmax':'x_max',\n",
    "#                       'ymin':'y_min',\n",
    "#                       'ymax':'y_max'\n",
    "#                       },inplace=True)\n",
    "\n",
    "# df_gt.to_csv(path_gt,index=False,sep=',')\n",
    "\n",
    "Path(\"../savmap_dataset_v2/train_splits\").mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "!python ./tools/patcher.py ../savmap_dataset_v2/images 640 640 100 \\\n",
    "    ../savmap_dataset_v2/train_splits \\\n",
    "    -csv ../savmap_dataset_v2/gt.csv \\\n",
    "    -min 0.0 -all False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling emtpy and non empty images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample training data\n",
    "\n",
    "directory = Path(r\"../savmap_dataset_v2/train/\")\n",
    "labels = directory/'labels'\n",
    "dest_images = directory/'images_nonempty'\n",
    "source_images = directory/'images'\n",
    "\n",
    "# create destination images\n",
    "dest_images.mkdir(exist_ok=True)\n",
    "\n",
    "# move non empty images\n",
    "# -- Uncomment to run > Be careful\n",
    "# for file in labels.iterdir():\n",
    "#     img_name = file.name.split('.')[0]+'.JPG'\n",
    "#     if (source_images/img_name).exists():\n",
    "#         os.rename(src=source_images/img_name,\n",
    "#                 dst=dest_images/img_name)\n",
    "\n",
    "# # and empty \n",
    "# -- Uncomment to run > Be careful!!\n",
    "# num_non_empty = len(list(labels.iterdir()))\n",
    "# num_empty_target = num_non_empty\n",
    "# empty_images = list(source_images.iterdir())\n",
    "# random.seed(41) # seeding for reproducibility\n",
    "# random.shuffle(empty_images) # shuffle\n",
    "# for file in empty_images[:num_empty_target]:\n",
    "#     img_name = file.name\n",
    "#     os.rename(src=file,\n",
    "#                 dst=dest_images/img_name)\n",
    "\n",
    "# rename folders\n",
    "# os.rename(src=source_images,dst=directory/'images_empty')\n",
    "# os.rename(src=dest_images,dst=directory/'images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list((directory/'images_empty').iterdir())),\\\n",
    "    len(list((directory/\"images\").iterdir())),\\\n",
    "        len(list((directory/\"labels\").iterdir()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from animaloc.datasets import CSVDataset\n",
    "from animaloc.data.transforms import MultiTransformsWrapper, DownSample, PointsToMask, FIDT\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_size = 640\n",
    "# num_classes = 7\n",
    "down_ratio = 1\n",
    "\n",
    "val_dataset = CSVDataset(\n",
    "    csv_file = '../wildlife_localizer_data/val/gt.csv',\n",
    "    root_dir = '../wildlife_localizer_data/val',\n",
    "    albu_transforms = [A.Normalize(p=1.0)],\n",
    "    end_transforms = [DownSample(down_ratio=down_ratio, anno_type='bbox')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.anno_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('../wildlife_localizer_data/val/gt.csv')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(labels['labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset = val_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = np.transpose(images.numpy(),(1,2,0))\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating YOLO labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images dimensions\n",
    "height,width = 640, 640\n",
    "\n",
    "# Create label directory\n",
    "for directory in Path(\"../wildlife_localizer_data/\").iterdir():\n",
    "    if not directory.is_dir():\n",
    "        continue \n",
    "    labels_dir = Path(os.path.join(directory,'labels'))\n",
    "    labels_dir.mkdir(exist_ok=True,parents=False) # create directory if it does not exist\n",
    "    labels = pd.read_csv(os.path.join(directory,'gt.csv'))\n",
    "    #-- Saving labels in YOLO format\n",
    "    for img_filename,df_group in tqdm(labels.groupby(by='images'),desc=directory.name):\n",
    "        df_group['width'] = (df_group['x_max'] - df_group['x_min'])/width \n",
    "        df_group['height'] = (df_group['y_max'] - df_group['y_min'])/height\n",
    "        df_group['x'] = (0.5*(df_group['x_min'] + df_group['x_max']))/width # x center\n",
    "        df_group['y'] = (0.5*(df_group['y_min'] + df_group['y_max']))/height # y center\n",
    "        df_group['labels'] = 0 \n",
    "\n",
    "        # print('\\n\\n',directory.name,'\\n',df_group[['labels','x','y','width','height']])\n",
    "        # break\n",
    "\n",
    "        # uncomment to save labels files\n",
    "        labels_filename     = img_filename.split('.')[0] + '.txt'\n",
    "        if len(df_group)>0:\n",
    "            cols = ['labels','x','y','width','height']\n",
    "            df_group[cols].to_csv(os.path.join(labels_dir,labels_filename),\n",
    "                                    sep=\" \",\n",
    "                                    header=False,\n",
    "                                    index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 2750/2750 [00:09<00:00, 303.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# for savmap data\n",
    "directory = Path(\"../savmap_dataset_v2/train/\")\n",
    "labels_dir = Path(os.path.join(directory,'labels'))\n",
    "labels_dir.mkdir(exist_ok=True,parents=False) # create directory if it does not exist\n",
    "labels = pd.read_csv(os.path.join(directory,'gt.csv'))\n",
    "#-- Saving labels in YOLO format\n",
    "for img_filename,df_group in tqdm(labels.groupby(by='images'),desc=directory.name):\n",
    "    df_group['width'] = (df_group['x_max'] - df_group['x_min'])/width \n",
    "    df_group['height'] = (df_group['y_max'] - df_group['y_min'])/height\n",
    "    df_group['x'] = (0.5*(df_group['x_min'] + df_group['x_max']))/width # x center\n",
    "    df_group['y'] = (0.5*(df_group['y_min'] + df_group['y_max']))/height # y center\n",
    "    df_group['labels'] = 0 \n",
    "\n",
    "    # print('\\n\\n',directory.name,'\\n',df_group[['labels','x','y','width','height']])\n",
    "    # break\n",
    "\n",
    "    # uncomment to save labels files\n",
    "    labels_filename = img_filename.split('.')[0] + '.txt'\n",
    "    if len(df_group)>0:\n",
    "        cols = ['labels','x','y','width','height']\n",
    "        df_group[cols].to_csv(os.path.join(labels_dir,labels_filename),\n",
    "                                    sep=\" \",\n",
    "                                    header=False,\n",
    "                                    index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30607, 2750, 33357)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Control splitting\n",
    "num_missing = 0\n",
    "num_found = 0\n",
    "num_total = 0\n",
    "for path in Path(\"../savmap_dataset_v2/train/images\").iterdir():\n",
    "    filename = path.name.split('.')[0]\n",
    "    labelpath = Path(\"../savmap_dataset_v2/train/labels\")/(filename + '.txt')\n",
    "    num_total += 1\n",
    "    if not labelpath.exists():\n",
    "        num_missing += 1\n",
    "    else:\n",
    "        num_found += 1\n",
    "\n",
    "num_missing,num_found,num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(os.path.join(directory,'gt.csv'))\n",
    "labels['labels'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Savmap data\n",
    "Saving bounding boxes in VOC format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import torch \n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.ops import nms\n",
    "from torchvision.transforms import PILToTensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_path = Path(\"../savmap_dataset_v2/savmap_annotations_2014.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gpd.read_file(annotations_path)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[data['IMAGEUUID']=='0a3ed15cfab4453795564140e8fde8ba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uuid = '0a3ed15cfab4453795564140e8fde8ba'\n",
    "# polygons = data.loc[data['IMAGEUUID']==uuid,'geometry']\n",
    "# polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = dict()\n",
    "count = 0\n",
    "pil_to_tensor = PILToTensor()\n",
    "for uuid in tqdm(np.unique(data.IMAGEUUID),desc='Getting bbox'):\n",
    "\n",
    "    # identifier, filenmae w/o suffix\n",
    "    uuid = str(uuid)\n",
    "\n",
    "    # load img as tensor\n",
    "    path_to_img = f\"../savmap_dataset_v2/images/{uuid}.JPG\"\n",
    "    img_pil = Image.open(path_to_img)\n",
    "    img_tensor = pil_to_tensor(img_pil)\n",
    "\n",
    "    # get boxes\n",
    "    polygons = data.loc[data['IMAGEUUID']==uuid,'geometry']\n",
    "    boxes = np.array([list(polygon.bounds) for polygon in polygons])\n",
    "    boxes = torch.from_numpy(boxes).float()\n",
    "\n",
    "    # apply non max suppression o discard overlaping polygons\n",
    "    areas = abs((boxes[:,2] - boxes[:,0])*(boxes[:,3] - boxes[:,1]))\n",
    "    indices = nms(boxes=boxes,\n",
    "                  scores= 1/areas, # discarding larger bbox when they overlap\n",
    "                  iou_threshold=0.1)\n",
    "    bbox = boxes[indices].numpy()\n",
    "\n",
    "    # save bbox\n",
    "    start, end = count, count+bbox.shape[0]\n",
    "    for idx,i in enumerate(range(start, end)):\n",
    "        bboxes[i] = [uuid,] + bbox[idx].tolist()\n",
    "    count = end\n",
    "# print('retained bbox indes:',indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['filename','x_min','y_min','x_max','y_max']\n",
    "gt_bboxes = pd.DataFrame.from_dict(data=bboxes,\n",
    "                       orient='index',\n",
    "                       columns=columns)\n",
    "\n",
    "for col in columns:\n",
    "    if col != 'images':\n",
    "        gt_bboxes[col] = gt_bboxes[col].apply(int)\n",
    "    else:\n",
    "        gt_bboxes[col] = gt_bboxes[col].apply(lambda x: f\"{x}.JPG\")\n",
    "\n",
    "gt_bboxes['labels'] = 0 # class\n",
    "gt_bboxes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discarding invalid coordinates\n",
    "# gt_bboxes.loc[gt_bboxes.min(axis=1,numeric_only=True)>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: drawing bounding boxes\n",
    "\n",
    "# load img as tensor\n",
    "filename = gt_bboxes['filename'].sample(1).iloc[0]\n",
    "path_to_img = f\"../savmap_dataset/{filename}\"\n",
    "img_pil = Image.open(path_to_img)\n",
    "img_tensor = PILToTensor()(img_pil)\n",
    "\n",
    "boxes = gt_bboxes.loc[gt_bboxes['filename'] == filename, ['x_min','y_min','x_max','y_max']].to_numpy()\n",
    "boxes = torch.from_numpy(boxes)\n",
    "\n",
    "img_with_box = draw_bounding_boxes(img_tensor,\n",
    "                                   boxes=boxes,\n",
    "                                   colors=\"red\",\n",
    "                                   width=5).numpy().transpose((1,2,0))\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.imshow(img_with_box)\n",
    "plt.title(filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from animaloc.data import ImageToPatches\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from sahi import AutoDetectionModel\n",
    "from sahi.models.yolov8 import Yolov8DetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.predict import get_sliced_prediction, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "path_to_weights = Path(r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\yolov8.kaza.pt\")\n",
    "\n",
    "# path_to_weights = Path('../yolo-runs/exp4/weights/best.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = YOLO('yolov8s.pt',task='detect').load(path_to_weights)\n",
    "\n",
    "model = YOLO(path_to_weights,task='detect')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "# load image\n",
    "dir_images = Path(r\"D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Farm\\DJI_202310040946_001_KapiriFarm1\").glob('*.jpg')\n",
    "img = Image.open(next(dir_images))\n",
    "\n",
    "patcher = ImageToPatches(img,size=(640,640),overlap=0)\n",
    "patches = patcher.make_patches()\n",
    "\n",
    "# model.predict()\n",
    "print(len(patcher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = patcher.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([117, 3, 640, 640])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 5.9ms\n",
      "1: 640x640 (no detections), 5.9ms\n",
      "2: 640x640 (no detections), 5.9ms\n",
      "3: 640x640 (no detections), 5.9ms\n",
      "4: 640x640 (no detections), 5.9ms\n",
      "5: 640x640 (no detections), 5.9ms\n",
      "6: 640x640 (no detections), 5.9ms\n",
      "7: 640x640 (no detections), 5.9ms\n",
      "8: 640x640 (no detections), 5.9ms\n",
      "9: 640x640 (no detections), 5.9ms\n",
      "10: 640x640 1 detection, 5.9ms\n",
      "11: 640x640 1 detection, 5.9ms\n",
      "12: 640x640 (no detections), 5.9ms\n",
      "13: 640x640 (no detections), 5.9ms\n",
      "14: 640x640 (no detections), 5.9ms\n",
      "15: 640x640 (no detections), 5.9ms\n",
      "16: 640x640 (no detections), 5.9ms\n",
      "17: 640x640 (no detections), 5.9ms\n",
      "18: 640x640 (no detections), 5.9ms\n",
      "19: 640x640 (no detections), 5.9ms\n",
      "20: 640x640 (no detections), 5.9ms\n",
      "21: 640x640 (no detections), 5.9ms\n",
      "22: 640x640 1 detection, 5.9ms\n",
      "23: 640x640 (no detections), 5.9ms\n",
      "24: 640x640 (no detections), 5.9ms\n",
      "25: 640x640 (no detections), 5.9ms\n",
      "26: 640x640 (no detections), 5.9ms\n",
      "27: 640x640 (no detections), 5.9ms\n",
      "28: 640x640 (no detections), 5.9ms\n",
      "29: 640x640 (no detections), 5.9ms\n",
      "30: 640x640 (no detections), 5.9ms\n",
      "31: 640x640 (no detections), 5.9ms\n",
      "32: 640x640 (no detections), 5.9ms\n",
      "33: 640x640 (no detections), 5.9ms\n",
      "34: 640x640 (no detections), 5.9ms\n",
      "35: 640x640 (no detections), 5.9ms\n",
      "36: 640x640 1 detection, 5.9ms\n",
      "37: 640x640 (no detections), 5.9ms\n",
      "38: 640x640 (no detections), 5.9ms\n",
      "39: 640x640 (no detections), 5.9ms\n",
      "40: 640x640 (no detections), 5.9ms\n",
      "41: 640x640 (no detections), 5.9ms\n",
      "42: 640x640 (no detections), 5.9ms\n",
      "43: 640x640 (no detections), 5.9ms\n",
      "44: 640x640 (no detections), 5.9ms\n",
      "45: 640x640 (no detections), 5.9ms\n",
      "46: 640x640 (no detections), 5.9ms\n",
      "47: 640x640 (no detections), 5.9ms\n",
      "48: 640x640 (no detections), 5.9ms\n",
      "49: 640x640 1 detection, 5.9ms\n",
      "50: 640x640 2 detections, 5.9ms\n",
      "51: 640x640 2 detections, 5.9ms\n",
      "52: 640x640 (no detections), 5.9ms\n",
      "53: 640x640 (no detections), 5.9ms\n",
      "54: 640x640 (no detections), 5.9ms\n",
      "55: 640x640 (no detections), 5.9ms\n",
      "56: 640x640 (no detections), 5.9ms\n",
      "57: 640x640 (no detections), 5.9ms\n",
      "58: 640x640 (no detections), 5.9ms\n",
      "59: 640x640 (no detections), 5.9ms\n",
      "60: 640x640 (no detections), 5.9ms\n",
      "61: 640x640 (no detections), 5.9ms\n",
      "62: 640x640 (no detections), 5.9ms\n",
      "63: 640x640 1 detection, 5.9ms\n",
      "64: 640x640 1 detection, 5.9ms\n",
      "65: 640x640 (no detections), 5.9ms\n",
      "66: 640x640 (no detections), 5.9ms\n",
      "67: 640x640 (no detections), 5.9ms\n",
      "68: 640x640 1 detection, 5.9ms\n",
      "69: 640x640 (no detections), 5.9ms\n",
      "70: 640x640 (no detections), 5.9ms\n",
      "71: 640x640 (no detections), 5.9ms\n",
      "72: 640x640 (no detections), 5.9ms\n",
      "73: 640x640 1 detection, 5.9ms\n",
      "74: 640x640 (no detections), 5.9ms\n",
      "75: 640x640 1 detection, 5.9ms\n",
      "76: 640x640 (no detections), 5.9ms\n",
      "77: 640x640 (no detections), 5.9ms\n",
      "78: 640x640 (no detections), 5.9ms\n",
      "79: 640x640 (no detections), 5.9ms\n",
      "80: 640x640 (no detections), 5.9ms\n",
      "81: 640x640 (no detections), 5.9ms\n",
      "82: 640x640 (no detections), 5.9ms\n",
      "83: 640x640 (no detections), 5.9ms\n",
      "84: 640x640 (no detections), 5.9ms\n",
      "85: 640x640 (no detections), 5.9ms\n",
      "86: 640x640 (no detections), 5.9ms\n",
      "87: 640x640 (no detections), 5.9ms\n",
      "88: 640x640 (no detections), 5.9ms\n",
      "89: 640x640 (no detections), 5.9ms\n",
      "90: 640x640 1 detection, 5.9ms\n",
      "91: 640x640 (no detections), 5.9ms\n",
      "92: 640x640 (no detections), 5.9ms\n",
      "93: 640x640 (no detections), 5.9ms\n",
      "94: 640x640 (no detections), 5.9ms\n",
      "95: 640x640 (no detections), 5.9ms\n",
      "96: 640x640 (no detections), 5.9ms\n",
      "97: 640x640 (no detections), 5.9ms\n",
      "98: 640x640 (no detections), 5.9ms\n",
      "99: 640x640 1 detection, 5.9ms\n",
      "100: 640x640 (no detections), 5.9ms\n",
      "101: 640x640 (no detections), 5.9ms\n",
      "102: 640x640 (no detections), 5.9ms\n",
      "103: 640x640 1 detection, 5.9ms\n",
      "104: 640x640 (no detections), 5.9ms\n",
      "105: 640x640 (no detections), 5.9ms\n",
      "106: 640x640 (no detections), 5.9ms\n",
      "107: 640x640 (no detections), 5.9ms\n",
      "108: 640x640 (no detections), 5.9ms\n",
      "109: 640x640 (no detections), 5.9ms\n",
      "110: 640x640 (no detections), 5.9ms\n",
      "111: 640x640 1 detection, 5.9ms\n",
      "112: 640x640 (no detections), 5.9ms\n",
      "113: 640x640 (no detections), 5.9ms\n",
      "114: 640x640 (no detections), 5.9ms\n",
      "115: 640x640 (no detections), 5.9ms\n",
      "116: 640x640 (no detections), 5.9ms\n",
      "Speed: 0.0ms preprocess, 5.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "model = model.to('cuda')\n",
    "preds = model(patches.to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(patches[9].numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using sahi\n",
    "# detection_model = AutoDetectionModel.from_pretrained(\n",
    "#     model_type='yolov8',\n",
    "#     model_path=path_to_weights,\n",
    "#     image_size=640,\n",
    "#     confidence_threshold=0.3,\n",
    "#     device=\"cpu\", # or 'cuda:0'\n",
    "# )\n",
    "\n",
    "detection_model = Yolov8DetectionModel(model=model,\n",
    "                                    #    model_path=path_to_weights,\n",
    "                                      confidence_threshold=0.4,\n",
    "                                      device=\"cuda:0\" # or 'cuda:0', 'cpu')\n",
    ")\n",
    "detection_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing prediction on 150 number of slices.\n"
     ]
    }
   ],
   "source": [
    "result = get_sliced_prediction(img, \n",
    "                               detection_model,\n",
    "                               slice_height=640,\n",
    "                               slice_width=640,\n",
    "                               overlap_height_ratio=0.1,\n",
    "                               overlap_width_ratio=0.1,\n",
    "                               postprocess_type='NMS',\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.export_visuals(export_dir=\"../tmp/\",\n",
    "                      hide_labels=False,\n",
    "                      hide_conf=True,text_size=3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': None,\n",
       "  'bbox': [7637.795715332031,\n",
       "   2068.1595458984375,\n",
       "   170.678466796875,\n",
       "   186.65167236328125],\n",
       "  'score': 0.8245490789413452,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 31857},\n",
       " {'image_id': None,\n",
       "  'bbox': [7377.245300292969,\n",
       "   2517.1500854492188,\n",
       "   277.61212158203125,\n",
       "   178.13055419921875],\n",
       "  'score': 0.7311072945594788,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 49451},\n",
       " {'image_id': None,\n",
       "  'bbox': [7860.680755615234,\n",
       "   2935.8993225097656,\n",
       "   234.11785888671875,\n",
       "   139.11868286132812],\n",
       "  'score': 0.7070891261100769,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 32570},\n",
       " {'image_id': None,\n",
       "  'bbox': [7156.594665527344,\n",
       "   223.54080200195312,\n",
       "   40.53228759765625,\n",
       "   35.333099365234375],\n",
       "  'score': 0.6239803433418274,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 1432},\n",
       " {'image_id': None,\n",
       "  'bbox': [5663.6641845703125,\n",
       "   3673.0907592773438,\n",
       "   53.170440673828125,\n",
       "   63.14111328125],\n",
       "  'score': 0.5189309120178223,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 3357},\n",
       " {'image_id': None,\n",
       "  'bbox': [8053.195068359375,\n",
       "   2327.967185974121,\n",
       "   15.94451904296875,\n",
       "   51.07811737060547],\n",
       "  'score': 0.5071122646331787,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 814},\n",
       " {'image_id': None,\n",
       "  'bbox': [6597.439056396484,\n",
       "   2479.612060546875,\n",
       "   37.838470458984375,\n",
       "   58.503631591796875],\n",
       "  'score': 0.4968582093715668,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 2213},\n",
       " {'image_id': None,\n",
       "  'bbox': [7920.494186401367,\n",
       "   4080.0969848632812,\n",
       "   117.99549865722656,\n",
       "   45.593505859375],\n",
       "  'score': 0.47619524598121643,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 5379},\n",
       " {'image_id': None,\n",
       "  'bbox': [5581.495826721191,\n",
       "   3545.0350646972656,\n",
       "   29.422698974609375,\n",
       "   30.63946533203125],\n",
       "  'score': 0.4143674075603485,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 901},\n",
       " {'image_id': None,\n",
       "  'bbox': [2439.666015625,\n",
       "   3336.9193725585938,\n",
       "   96.5506591796875,\n",
       "   23.887054443359375],\n",
       "  'score': 0.4109806418418884,\n",
       "  'category_id': 0,\n",
       "  'category_name': 'detection',\n",
       "  'segmentation': [],\n",
       "  'iscrowd': 0,\n",
       "  'area': 2306}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_coco_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wildaidata-test', 'p1/p2/000113a692ba61cd55ea3acb9c2f9c41709710a1_S2.JPG')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parsing s3 url\n",
    "url = 's3://wildaidata-test/p1/p2/000113a692ba61cd55ea3acb9c2f9c41709710a1_S2.JPG'\n",
    "s3_img = Path(url)\n",
    "\n",
    "img_path = s3_img\n",
    "bucket = list(img_path.parents)[-3]\n",
    "bucket_name = str(bucket).split('/')[-1]\n",
    "filename = str(img_path).replace(f\"{str(bucket)}/\",'')\n",
    "\n",
    "bucket_name, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('wildaidata-test', 'p1/p2/000113a692ba61cd55ea3acb9c2f9c41709710a1_S2.JPG')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = urlparse(url, allow_fragments=False)\n",
    "bucket_name = r.netloc\n",
    "key = r.path.lstrip('/')\n",
    "bucket_name,key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-annotate data for Label studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.models.yolov8 import Yolov8DetectionModel\n",
    "from sahi.predict import get_sliced_prediction, predict\n",
    "from PIL import Image\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prediction(pred:dict,img_height:int,img_width:int):\n",
    "        # formatting the prediction to work with Label studio\n",
    "        x, y, width, height = pred['bbox']\n",
    "        label = pred['category_name']\n",
    "        score = pred['score']\n",
    "\n",
    "        template = {\n",
    "                    \"from_name\": \"label\",\n",
    "                    \"to_name\": \"image\",\n",
    "                    \"type\": \"rectanglelabels\",\n",
    "                    'value': {\n",
    "                        'rectanglelabels': [label],\n",
    "                        'x': x / img_width * 100,\n",
    "                        'y': y / img_height * 100,\n",
    "                        'width': width / img_width * 100,\n",
    "                        'height': height / img_height * 100\n",
    "                    },\n",
    "                    'score': score\n",
    "        }\n",
    "\n",
    "        return template\n",
    "\n",
    "def format_path(file_path:Path,root_directory:Path=Path(r\"D:\")):\n",
    "    relative_to_root = Path(os.path.relpath(file_path,root_directory)).as_posix()\n",
    "    label_studio_format = f\"/data/local-files/?d={root_directory.name}/{relative_to_root}\"\n",
    "    return label_studio_format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/local-files/?d=/PhD/Data per camp/Extra training data/Bushriver_drone_paul/DJI_0004.JPG'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_path(file_path=Path(r\"D:\\PhD\\Data per camp\\Extra training data\\Bushriver_drone_paul\\DJI_0004.JPG\"),\n",
    "            # root_directory=Path(r\"D:\\PhD\\Data per camp\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detection model\n",
    "detection_model = Yolov8DetectionModel(#model=model,\n",
    "                                       model_path=r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\yolov8.kaza.pt\",\n",
    "                                      confidence_threshold=0.4,\n",
    "                                      device=\"cuda\" # or 'cuda:0', 'cpu')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2366/2366 [2:18:18<00:00,  3.51s/it]  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# get pre-annotations \n",
    "# It assumes that all images have extension .JPG or .jpg\n",
    "pre_annotations = list()\n",
    "count = 0\n",
    "data_directory = Path(r\"D:\\PhD\\Data per camp\\Dry season\\Kapiri\\Camp 1, 4, 5\")\n",
    "all_images = data_directory.glob(\"**/*.jpg\",\n",
    "                                 case_sensitive=False)\n",
    "all_images = list(all_images)\n",
    "\n",
    "for img_path in tqdm(all_images):\n",
    "        count += 1\n",
    "        # read file\n",
    "        img = Image.open(img_path)\n",
    "        # create template\n",
    "        annotations = dict()\n",
    "        annotations[\"id\"] = count\n",
    "        annotations[\"data\"] = {\"image\":format_path(file_path=img_path)} \n",
    "        # get prediction on image\n",
    "        result = get_sliced_prediction(img,       \n",
    "                                        detection_model,\n",
    "                                        slice_height=512,\n",
    "                                        slice_width=512,\n",
    "                                        overlap_height_ratio=0.1,\n",
    "                                        overlap_width_ratio=0.1,\n",
    "                                        postprocess_type='NMS',\n",
    "                                        postprocess_match_metric='IOS',\n",
    "                                        verbose=0\n",
    "                                        )  \n",
    "        img_height = result.image_height\n",
    "        img_width = result.image_width\n",
    "        formatted_pred = [format_prediction(pred,\n",
    "                                                img_height=img_height,\n",
    "                                                img_width=img_width) for pred in result.to_coco_annotations()]\n",
    "        annotations[\"predictions\"] = [{\"model_version\":\"0.0.1\",\n",
    "                                       'result':formatted_pred\n",
    "                                       }\n",
    "                                       ]\n",
    "        # store pre-annotations\n",
    "        pre_annotations.append(annotations)\n",
    "\n",
    "\n",
    "# save as json\n",
    "save_path = \"_\".join([data_directory.name] + [parent.name for parent in data_directory.parents][:2]) + \".json\"\n",
    "save_path = Path(r\"C:\\Users\\Machine Learning\\Desktop\\workspace-wildAI\\Preannotated-data\") / save_path\n",
    "with open(save_path,\"w\") as file:\n",
    "    json.dump(pre_annotations,file,indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2061"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Farm_Kapiri_Dry season.json'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"-\".join([] + [parent.name for parent in data_directory.parents][:2])\n",
    "\"_\".join([data_directory.name] + [parent.name for parent in data_directory.parents][:2]) + \".json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2061"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre_annotations\n",
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as json\n",
    "# with open(\"./template_preannotations.json\",\"w\") as file:\n",
    "#     json.dump(pre_annotations,file,indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "herdnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
