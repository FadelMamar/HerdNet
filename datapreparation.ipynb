{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from pathlib import Path\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import PIL\n",
    "import torchvision\n",
    "import numpy\n",
    "import cv2\n",
    "import skimage\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from albumentations import PadIfNeeded\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from animaloc.data import ImageToPatches, PatchesBuffer, save_batch_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting data for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For GeoTiffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_patches(img_path:Path,dest_dir:Path,tilesize=512):\n",
    "\n",
    "    # create directory\n",
    "    if os.path.exists(dest_dir):\n",
    "        shutil.rmtree(dest_dir)\n",
    "        os.mkdir(dest_dir)\n",
    "        print(\"emptying directory:\", dest_dir)\n",
    "    else:\n",
    "        os.mkdir(dest_dir)\n",
    "        print(\"creating directory:\", dest_dir)\n",
    "    \n",
    "    # window reading with rasterio\n",
    "    handler = rasterio.open(img_path)\n",
    "    height, width = handler.meta['height'], handler.meta['width']\n",
    "    coordinates = dict()\n",
    "    count = 0\n",
    "    for i,j in tqdm(product(list(range(0,height,tilesize)),list(range(0,width,tilesize)))):\n",
    "        window = Window(j, i, tilesize, tilesize)\n",
    "        \n",
    "        try:\n",
    "            chunk = handler.read(window=window)\n",
    "            c,h,w = chunk.shape\n",
    "            xmin, xmax = j, j+w\n",
    "            ymin, ymax = i, i+h\n",
    "            x_center = 0.5*(xmin+xmax)\n",
    "            y_center = 0.5*(ymin+ymax)\n",
    "            n_unique = np.unique(chunk).size\n",
    "            if n_unique == 1:\n",
    "                continue\n",
    "            count += 1\n",
    "            filename = img_path.name.split('.')[0] + f\"-{j}-{i}.png\"\n",
    "            coordinates[count] = [xmin,xmax,ymin,ymax,x_center,y_center,filename]\n",
    "\n",
    "            # save to disk\n",
    "            chunk = np.transpose(chunk,(1,2,0))\n",
    "            skimage.io.imsave(dest_dir/filename,chunk)          \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Failed for\",(i,j),e)\n",
    "            pass\n",
    "\n",
    "    cols = ['xmin','xmax','ymin','ymax','x_center','y_center','filename']\n",
    "    coordinates = pd.DataFrame.from_dict(coordinates,\n",
    "                                        orient='index',\n",
    "                                        columns=cols)\n",
    "    coordinates.to_csv(dest_dir/f\"coordinates{img_path.name.split('.')[0]}.csv\",index=False)\n",
    "    handler.close()\n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emptying directory: ../annotation_data/camp6/150m_RGB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "126it [01:09,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# for datapath in Path(\"../annotation_data/camp6/\").iterdir():\n",
    "datapath = Path(\"../annotation_data/camp6/150m_RGB.tif\")\n",
    "dest_dir = datapath.parent/(datapath.name.split('.')[0])\n",
    "# coordinates =  save_patches(datapath,dest_dir,tilesize=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymin</th>\n",
       "      <th>ymax</th>\n",
       "      <th>x_center</th>\n",
       "      <th>y_center</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>512.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>150m_RGB-0-0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1024</td>\n",
       "      <td>2048</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>1536.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>150m_RGB-1024-0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2048</td>\n",
       "      <td>3072</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2560.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>150m_RGB-2048-0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3072</td>\n",
       "      <td>4096</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>3584.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>150m_RGB-3072-0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4096</td>\n",
       "      <td>5120</td>\n",
       "      <td>0</td>\n",
       "      <td>1024</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>512.0</td>\n",
       "      <td>150m_RGB-4096-0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>2048</td>\n",
       "      <td>3072</td>\n",
       "      <td>8192</td>\n",
       "      <td>8612</td>\n",
       "      <td>2560.0</td>\n",
       "      <td>8402.0</td>\n",
       "      <td>150m_RGB-2048-8192.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>3072</td>\n",
       "      <td>4096</td>\n",
       "      <td>8192</td>\n",
       "      <td>8612</td>\n",
       "      <td>3584.0</td>\n",
       "      <td>8402.0</td>\n",
       "      <td>150m_RGB-3072-8192.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>4096</td>\n",
       "      <td>5120</td>\n",
       "      <td>8192</td>\n",
       "      <td>8612</td>\n",
       "      <td>4608.0</td>\n",
       "      <td>8402.0</td>\n",
       "      <td>150m_RGB-4096-8192.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5120</td>\n",
       "      <td>6144</td>\n",
       "      <td>8192</td>\n",
       "      <td>8612</td>\n",
       "      <td>5632.0</td>\n",
       "      <td>8402.0</td>\n",
       "      <td>150m_RGB-5120-8192.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6144</td>\n",
       "      <td>7168</td>\n",
       "      <td>8192</td>\n",
       "      <td>8612</td>\n",
       "      <td>6656.0</td>\n",
       "      <td>8402.0</td>\n",
       "      <td>150m_RGB-6144-8192.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>116 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     xmin  xmax  ymin  ymax  x_center  y_center                filename\n",
       "1       0  1024     0  1024     512.0     512.0        150m_RGB-0-0.png\n",
       "2    1024  2048     0  1024    1536.0     512.0     150m_RGB-1024-0.png\n",
       "3    2048  3072     0  1024    2560.0     512.0     150m_RGB-2048-0.png\n",
       "4    3072  4096     0  1024    3584.0     512.0     150m_RGB-3072-0.png\n",
       "5    4096  5120     0  1024    4608.0     512.0     150m_RGB-4096-0.png\n",
       "..    ...   ...   ...   ...       ...       ...                     ...\n",
       "112  2048  3072  8192  8612    2560.0    8402.0  150m_RGB-2048-8192.png\n",
       "113  3072  4096  8192  8612    3584.0    8402.0  150m_RGB-3072-8192.png\n",
       "114  4096  5120  8192  8612    4608.0    8402.0  150m_RGB-4096-8192.png\n",
       "115  5120  6144  8192  8612    5632.0    8402.0  150m_RGB-5120-8192.png\n",
       "116  6144  7168  8192  8612    6656.0    8402.0  150m_RGB-6144-8192.png\n",
       "\n",
       "[116 rows x 7 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Images (jpg, rgb etc.)\n",
    "Using Herdnet code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/workspace/HerdNet\n"
     ]
    }
   ],
   "source": [
    "!pwd # current working dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buffer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [01:39<00:00,  1.11it/s]\n",
      "Exporting patches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 111/111 [06:00<00:00,  3.25s/it]\n"
     ]
    }
   ],
   "source": [
    "# splitting validation data\n",
    "Path(\"../general_dataset/val_splits\").mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "!python ./tools/patcher.py ../general_dataset/val 640 640 100 \\\n",
    "    ../general_dataset/val_splits \\\n",
    "    -csv ../general_dataset/groundtruth/csv/val_big_size_A_B_E_K_WH_WB.csv \\\n",
    "    -min 0.0 -all False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the buffer: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 928/928 [13:25<00:00,  1.15it/s]\n",
      "Exporting patches:  27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                | 248/928 [13:09<36:35,  3.23s/it]"
     ]
    }
   ],
   "source": [
    "# splitting training data\n",
    "Path(\"../general_dataset/train_splits\").mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "!python ./tools/patcher.py ../general_dataset/train 640 640 100 \\\n",
    "    ../general_dataset/train_splits \\\n",
    "    -csv ../general_dataset/groundtruth/csv/train_big_size_A_B_E_K_WH_WB.csv \\\n",
    "    -min 0.0 -all False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting test data\n",
    "Path(\"../general_dataset/test_splits\").mkdir(exist_ok=True,parents=True)\n",
    "\n",
    "!python ./tools/patcher.py ../general_dataset/test 640 640 100 \\\n",
    "    ../general_dataset/test_splits \\\n",
    "    -csv ../general_dataset/groundtruth/csv/test_big_size_A_B_E_K_WH_WB.csv \\\n",
    "    -min 0.0 -all False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample training data\n",
    "# non empty\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "directory = Path(r\"C:\\Users\\fadel\\OneDrive\\Bureau\\WILD-AI\\wildlife_localizer_data\\train\")\n",
    "labels = directory/'labels'\n",
    "dest_images = directory/'images_1e-1empty_nonempty'\n",
    "source_images = directory/'images'\n",
    "\n",
    "for file in labels.iterdir():\n",
    "    img_name = file.name.split('.')[0]+'.JPG'\n",
    "    if (source_images/img_name).exists():\n",
    "        os.rename(src=source_images/img_name,\n",
    "                dst=dest_images/img_name)\n",
    "        \n",
    "# and empty \n",
    "num_non_empty = len(list(labels.iterdir()))\n",
    "num_empty_target = num_non_empty\n",
    "empty_images = list(source_images.iterdir())\n",
    "random.seed(41) # seeding for reproducibility\n",
    "random.shuffle(empty_images) # shuffle\n",
    "for file in empty_images[:num_empty_target]:\n",
    "    img_name = file.name\n",
    "    os.rename(src=file,\n",
    "                dst=dest_images/img_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from animaloc.datasets import CSVDataset\n",
    "from animaloc.data.transforms import MultiTransformsWrapper, DownSample, PointsToMask, FIDT\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patch_size = 640\n",
    "# num_classes = 7\n",
    "down_ratio = 1\n",
    "\n",
    "val_dataset = CSVDataset(\n",
    "    csv_file = '../wildlife_localizer_data/val/gt.csv',\n",
    "    root_dir = '../wildlife_localizer_data/val',\n",
    "    albu_transforms = [A.Normalize(p=1.0)],\n",
    "    end_transforms = [DownSample(down_ratio=down_ratio, anno_type='bbox')]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BoundingBox'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.anno_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>base_images</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>x_max</th>\n",
       "      <th>y_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>005c952ba7a612c40986806cc84a87e1573ef4f2_13.JPG</td>\n",
       "      <td>6</td>\n",
       "      <td>005c952ba7a612c40986806cc84a87e1573ef4f2.JPG</td>\n",
       "      <td>243</td>\n",
       "      <td>255</td>\n",
       "      <td>283</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005c952ba7a612c40986806cc84a87e1573ef4f2_15.JPG</td>\n",
       "      <td>6</td>\n",
       "      <td>005c952ba7a612c40986806cc84a87e1573ef4f2.JPG</td>\n",
       "      <td>399</td>\n",
       "      <td>254</td>\n",
       "      <td>452</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>005c952ba7a612c40986806cc84a87e1573ef4f2_23.JPG</td>\n",
       "      <td>6</td>\n",
       "      <td>005c952ba7a612c40986806cc84a87e1573ef4f2.JPG</td>\n",
       "      <td>293</td>\n",
       "      <td>283</td>\n",
       "      <td>339</td>\n",
       "      <td>308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>031833f31b1622ec6701b7433a1664231f401d73_4.JPG</td>\n",
       "      <td>6</td>\n",
       "      <td>031833f31b1622ec6701b7433a1664231f401d73.JPG</td>\n",
       "      <td>183</td>\n",
       "      <td>144</td>\n",
       "      <td>226</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>031833f31b1622ec6701b7433a1664231f401d73_4.JPG</td>\n",
       "      <td>6</td>\n",
       "      <td>031833f31b1622ec6701b7433a1664231f401d73.JPG</td>\n",
       "      <td>118</td>\n",
       "      <td>275</td>\n",
       "      <td>157</td>\n",
       "      <td>295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            images  labels  \\\n",
       "0  005c952ba7a612c40986806cc84a87e1573ef4f2_13.JPG       6   \n",
       "1  005c952ba7a612c40986806cc84a87e1573ef4f2_15.JPG       6   \n",
       "2  005c952ba7a612c40986806cc84a87e1573ef4f2_23.JPG       6   \n",
       "3   031833f31b1622ec6701b7433a1664231f401d73_4.JPG       6   \n",
       "4   031833f31b1622ec6701b7433a1664231f401d73_4.JPG       6   \n",
       "\n",
       "                                    base_images  x_min  y_min  x_max  y_max  \n",
       "0  005c952ba7a612c40986806cc84a87e1573ef4f2.JPG    243    255    283    307  \n",
       "1  005c952ba7a612c40986806cc84a87e1573ef4f2.JPG    399    254    452    302  \n",
       "2  005c952ba7a612c40986806cc84a87e1573ef4f2.JPG    293    283    339    308  \n",
       "3  031833f31b1622ec6701b7433a1664231f401d73.JPG    183    144    226    181  \n",
       "4  031833f31b1622ec6701b7433a1664231f401d73.JPG    118    275    157    295  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('../wildlife_localizer_data/val/gt.csv')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(labels['labels'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DataLoader(dataset = val_dataset, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = val_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': tensor([6]),\n",
       " 'base_images': ['005c952ba7a612c40986806cc84a87e1573ef4f2.JPG'],\n",
       " 'boxes': tensor([[243., 255., 283., 307.]]),\n",
       " 'image_id': [0],\n",
       " 'image_name': ['005c952ba7a612c40986806cc84a87e1573ef4f2_13.JPG']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img = np.transpose(images.numpy(),(1,2,0))\n",
    "# plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating YOLO labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test:   0%|          | 0/1633 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1633/1633 [00:06<00:00, 240.74it/s]\n",
      "val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 723/723 [00:03<00:00, 198.82it/s]\n",
      "train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5186/5186 [00:22<00:00, 232.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Images dimensions\n",
    "height,width = 640, 640\n",
    "\n",
    "# Create label directory\n",
    "for directory in Path(\"../wildlife_localizer_data/\").iterdir():\n",
    "    if not directory.is_dir():\n",
    "        continue \n",
    "    labels_dir = Path(os.path.join(directory,'labels'))\n",
    "    labels_dir.mkdir(exist_ok=True,parents=False) # create directory if it does not exist\n",
    "    labels = pd.read_csv(os.path.join(directory,'gt.csv'))\n",
    "    #-- Saving labels in YOLO format\n",
    "    for img_filename,df_group in tqdm(labels.groupby(by='images'),desc=directory.name):\n",
    "        df_group['width'] = (df_group['x_max'] - df_group['x_min'])/width \n",
    "        df_group['height'] = (df_group['y_max'] - df_group['y_min'])/height\n",
    "        df_group['x'] = (0.5*(df_group['x_min'] + df_group['x_max']))/width # x center\n",
    "        df_group['y'] = (0.5*(df_group['y_min'] + df_group['y_max']))/height # y center\n",
    "        df_group['labels'] = 0 \n",
    "\n",
    "        # print('\\n\\n',directory.name,'\\n',df_group[['labels','x','y','width','height']])\n",
    "        # break\n",
    "\n",
    "        # uncomment to save labels files\n",
    "        labels_filename     = img_filename.split('.')[0] + '.txt'\n",
    "        if len(df_group)>0:\n",
    "            cols = ['labels','x','y','width','height']\n",
    "            df_group[cols].to_csv(os.path.join(labels_dir,labels_filename),\n",
    "                                    sep=\" \",\n",
    "                                    header=False,\n",
    "                                    index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control splitting\n",
    "num_missing = 0\n",
    "num_found = 0\n",
    "num_total = 0\n",
    "for path in Path(\"../wildlife_localizer_data/train/images\").iterdir():\n",
    "    filename = path.name.split('.')[0]\n",
    "    labelpath = Path(\"../wildlife_localizer_data/train/labels\")/(filename + '.txt')\n",
    "    num_total += 1\n",
    "    if not labelpath.exists():\n",
    "        num_missing += 1\n",
    "    else:\n",
    "        num_found += 1\n",
    "\n",
    "num_missing,num_found,num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 3, 2, 5, 4, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv(os.path.join(directory,'gt.csv'))\n",
    "labels['labels'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with YOLO\n",
    "Using ultralytics library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a YOLO dataset using ultralytics > Debugging\n",
    "import yaml\n",
    "with open(\"../wildlife_localizer_data/data_config.yaml\", 'r') as file:\n",
    "    data_yaml = yaml.safe_load(file)\n",
    "dataset = ultralytics.data.YOLODataset(data=data_yaml,\n",
    "                                       img_path = \"../wildlife_localizer_data/train/images\",\n",
    "                                       use_keypoints=False,\n",
    "                                       single_cls=True,\n",
    "                                       use_segments=False)\n",
    "# view dataset\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(225, 3157200, 0, 8.8575488)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\",task='detect')  # load a pretrained model (recommended for training)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n"
     ]
    }
   ],
   "source": [
    "!wandb offline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.209 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.208 ðŸš€ Python-3.11.5 torch-2.0.1+cu117 CPU (Intel Xeon Platinum 8175M 2.50GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=../wildlife_localizer_data/data_config.yaml, epochs=1, patience=50, batch=8, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ubuntu/workspace/wildlife_localizer_data/train/labels... 5186 images, 72156 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77342/77342 [00:21<00:00, 3610.67it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/ubuntu/workspace/wildlife_localizer_data/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ubuntu/workspace/wildlife_localizer_data/val/labels... 723 images, 8450 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9173/9173 [00:02<00:00, 3904.75it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/ubuntu/workspace/wildlife_localizer_data/val/labels.cache\n",
      "Plotting labels to runs/detect/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/1         0G      1.017      39.05     0.6895          0        640:   3%|â–Ž         | 262/9668 [25:57<15:31:52,  5.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/workspace/HerdNet/datapreparation.ipynb Cell 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/workspace/HerdNet/datapreparation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Use the model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/workspace/HerdNet/datapreparation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../wildlife_localizer_data/data_config.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/workspace/HerdNet/datapreparation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m             epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/workspace/HerdNet/datapreparation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m             imgsz\u001b[39m=\u001b[39;49m\u001b[39m640\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/workspace/HerdNet/datapreparation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m             batch\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2/home/ubuntu/workspace/HerdNet/datapreparation.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m             single_cls\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,)  \u001b[39m# train the model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/engine/model.py:338\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    337\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    339\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/engine/trainer.py:190\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/engine/trainer.py:337\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamp):\n\u001b[1;32m    336\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 337\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(batch)\n\u001b[1;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m RANK \u001b[39m!=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    339\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m world_size\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/nn/tasks.py:41\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/nn/tasks.py:211\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit_criterion()\n\u001b[0;32m--> 211\u001b[0m preds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(batch[\u001b[39m'\u001b[39;49m\u001b[39mimg\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39mif\u001b[39;00m preds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m preds\n\u001b[1;32m    212\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(preds, batch)\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/nn/tasks.py:42\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, \u001b[39mdict\u001b[39m):  \u001b[39m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss(x, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/nn/tasks.py:59\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_augment(x)\n\u001b[0;32m---> 59\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/nn/tasks.py:79\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m---> 79\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m     80\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/nn/modules/head.py:46\u001b[0m, in \u001b[0;36mDetect.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m shape \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape  \u001b[39m# BCHW\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnl):\n\u001b[0;32m---> 46\u001b[0m     x[i] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2[i](x[i]), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcv3[i](x[i])), \u001b[39m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m     48\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/ultralytics/nn/modules/conv.py:36\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Apply convolution, batch normalization and activation to input tensor.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)))\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/herdnet/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use the model\n",
    "model.train(data=\"../wildlife_localizer_data/data_config.yaml\",\n",
    "            epochs=100,\n",
    "            imgsz=640,\n",
    "            batch=64,\n",
    "            single_cls=True,\n",
    "            patience=20,\n",
    "            device=0,\n",
    "            resume=False,\n",
    "            dropout=0.1,\n",
    "            iou=0.5,\n",
    "            optimizer='Adam',\n",
    "            pretrained=True,\n",
    "            cos_lr=False,\n",
    "            lr0=1e-3,\n",
    "            lrf=1e-5)  # train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics = model.val()  # evaluate model performance on the validation set\n",
    "# results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
    "# path = model.export(format=\"onnx\")  # export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "herdnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
